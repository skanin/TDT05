{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(open(\"run_mice.R\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: use read.table and write.table instead of read.csv and write.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.naive_bayes import GaussianNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "X = df_train.drop(['target', 'id'], axis=1)\n",
    "y = df_train['target']\n",
    "\n",
    "X_test = df_test.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_columns = ['f2', 'f3', 'f13', 'f18', 'f20', 'f26']\n",
    "ordinal_columns = ['f6', 'f4', 'f8', 'f16', 'f17', 'f19', 'f21', 'f25']\n",
    "categorical_columns = ['f1', 'f5', 'f7', 'f9', 'f11', 'f24']\n",
    "ordinal_cat_columns = ['f0', 'f12', 'f23', 'f27']\n",
    "binary_columns = ['f10', 'f28', 'f22', 'f14']\n",
    "all_columns = ['f' + str(i) for i in range(0, 29)]\n",
    "removed_cols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_hex(df: pd.DataFrame) -> None:\n",
    "    def conv_hex_map(x):\n",
    "        try:\n",
    "            return int(x, 16)\n",
    "        except ValueError as e:\n",
    "            return np.nan\n",
    "        except TypeError as e:\n",
    "            return np.nan\n",
    "\n",
    "    for col in hex_columns:\n",
    "        if col not in list(df.columns):\n",
    "            continue\n",
    "        col_loc = list(df.columns).index(col)\n",
    "        df[col] = df[col].apply(lambda x: conv_hex_map(x))\n",
    "\n",
    "\n",
    "def conv_bool(df: pd.DataFrame):\n",
    "    def conv_bool_map(x):\n",
    "        try:\n",
    "            if not type(x) == str:\n",
    "                return x\n",
    "            if x.lower() == 'f':\n",
    "                return 0\n",
    "            elif x.lower() == 't':\n",
    "                return 1\n",
    "            return x\n",
    "        except Exception as e:\n",
    "            return np.nan\n",
    "    df['f14'] = df['f14'].apply(lambda x: conv_bool_map(x))\n",
    "\n",
    "\n",
    "def conv_binary(df: pd.DataFrame):\n",
    "    def binaryToDecimal(binary):\n",
    "        try:\n",
    "            binary = int(binary)\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "        binary1 = binary\n",
    "        decimal, i, n = 0, 0, 0\n",
    "        while(binary != 0):\n",
    "            dec = binary % 10\n",
    "            decimal = decimal + dec * pow(2, i)\n",
    "            binary = binary//10\n",
    "            i += 1\n",
    "        return decimal\n",
    "    for col in binary_columns:\n",
    "        if col not in list(df.columns):\n",
    "            continue\n",
    "    df[col] = df[col].apply(lambda x: binaryToDecimal(x))\n",
    "    \n",
    "def remove_duplicate_columns(df: pd.DataFrame) -> None:\n",
    "    cols_to_drop = []\n",
    "    cols = list(df.columns)\n",
    "    for col in df.columns:\n",
    "        if col in cols:\n",
    "            cols.remove(col)\n",
    "        for col2 in cols:\n",
    "            if df[col].equals(df[col2]):\n",
    "                cols_to_drop.append(col2)\n",
    "\n",
    "    df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "def transform_categorical(df: pd.DataFrame, test=False) -> None:\n",
    "    cols = list(set(df.columns).intersection(set(categorical_columns).union(set(ordinal_cat_columns))))\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype('category')\n",
    "        df[col] = df[col].cat.codes\n",
    "\n",
    "def remove_duplicates(df: pd.DataFrame) -> None:\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "def conv_columns(df: pd.DataFrame, test=False) -> None:\n",
    "    conv_hex(df)\n",
    "    conv_bool(df)\n",
    "    conv_binary(df)\n",
    "    return transform_categorical(df, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mice():\n",
    "    infile = 'train_processed.csv'\n",
    "    outfile = 'train_imputed.csv'\n",
    "    # np.savetxt(infile, data.to_numpy().tolist(), delimiter=\",\")\n",
    "    os.system(f'type nul > {outfile}')\n",
    "    os.system('\"C:\\Program Files\\R\\R-4.1.1\\bin\\Rscript.exe\" --vanilla run_mice.R %s %s' % (infile, outfile))\n",
    "    data_imputed = pd.read_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, test=False):\n",
    "    if test:\n",
    "        df.loc[26648, 'f9'] = np.nan\n",
    "        df.loc[20956, 'f15'] = np.nan\n",
    "        df.loc[21034, 'f15'] = np.nan\n",
    "        \n",
    "    remove_duplicate_columns(df)\n",
    "    remove_duplicates(df)\n",
    "\n",
    "    conv_columns(df, test)\n",
    "    \n",
    "    if test:\n",
    "        df.to_csv('./test_processed.csv', index=False)\n",
    "    else:\n",
    "        df.to_csv('./train_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess(df_train)\n",
    "preprocess(df_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(100*(X_test_pre.isnull().sum()/len(X_test_pre.index)),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "for col in X_train_pre:\n",
    "    if X_train_pre[col].isnull().sum() > 0:\n",
    "        X_train_pre[col].fillna(X_train_pre[col].mean(), inplace=True)\n",
    "\n",
    "sm = SMOTE(sampling_strategy='minority', random_state=7)\n",
    "X_cols = X_train_pre.columns\n",
    "\n",
    "X_train_pre, y_train_pre = sm.fit_resample(X_train_pre, y_train)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=X_cols)\n",
    "\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split(X : pd.DataFrame, y : pd.DataFrame, size=.2):\n",
    "    return train_test_split(X, y, test_size=size, stratify=y)\n",
    "\n",
    "def fit(data : pd.DataFrame, labels : pd.DataFrame, classifier : any, eval_pool = None) -> None:\n",
    "    if eval_pool is not None:\n",
    "        classifier.fit(data, labels, eval_set=eval_pool)\n",
    "    else:\n",
    "        classifier.fit(data, labels)\n",
    "\n",
    "def predict_proba(classifier : any, test : pd.DataFrame) -> np.array:\n",
    "    return classifier.predict_proba(test)\n",
    "\n",
    "def predict(classifier : any, test : pd.DataFrame) -> np.array:\n",
    "    return classifier.predict(test)\n",
    "\n",
    "def print_accuracy(pred : np.array, test : pd.DataFrame, name : str):\n",
    "    print(name + ' Model accuracy score: {0:0.4f}'.format(roc_auc_score(test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_data = pd.read_csv('train_imputed.csv')\n",
    "\n",
    "X_train_pre = imp_data.drop(['target', 'id'], axis=1)\n",
    "y_train = imp_data['target']\n",
    "\n",
    "df_test = pd.read_csv('test_imputed.csv')\n",
    "X_test_pre = df_test.drop('id', axis=1)\n",
    "\n",
    "train_cat = list(set(X_train_split.columns).intersection(set(categorical_columns)))\n",
    "\n",
    "for col in X_train_pre:\n",
    "    if X_train_pre[col].isnull().sum() > 0:\n",
    "        X_train_pre[col].fillna(method='bfill', inplace=True)\n",
    "    \n",
    "    # if col in train_cat:\n",
    "        # X_train_pre[col] = X_train_pre[col].astype('category')\n",
    "        \n",
    "for col in X_test_pre:\n",
    "    if X_test_pre[col].isnull().sum() > 0:\n",
    "        X_test_pre[col].fillna(method='bfill', inplace=True)\n",
    "    \n",
    "    # if col in train_cat:\n",
    "        # X_train_pre[col] = X_train_pre[col].astype('category')\n",
    "        \n",
    "remove_duplicate_columns(X_train_pre)\n",
    "remove_duplicate_columns(X_test_pre)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# cols_to_transform = list(set(X_train_pre.columns) - set(train_cat))\n",
    "\n",
    "# X_train_pre[cols_to_transform] = scaler.fit_transform(X_train_pre[cols_to_transform], y_train)\n",
    "# X_test_pre[cols_to_transform] = scaler.transform(X_test_pre[cols_to_transform])\n",
    "\n",
    "X_train_split, X_test_split, y_train_split, y_test_split = split(X_train_pre, y_train, size=.2)\n",
    "        \n",
    "# X_train_pre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "train_cat = list(set(X_train_split.columns).intersection(set(categorical_columns)))\n",
    "test_cat = list(set(X_test_split.columns).intersection(set(categorical_columns)))\n",
    "cats = []\n",
    "for col in train_cat:\n",
    "    cats.append(X_train_pre.columns.get_loc(col))\n",
    "    \n",
    "print(cats)\n",
    "print(train_cat)\n",
    "print(test_cat)\n",
    "\n",
    "train_dataset = Pool(X_train_split ,y_train_split, cat_features=train_cat)\n",
    "test_dataset = Pool(X_test_split, y_test_split, cat_features=test_cat)\n",
    "\n",
    "model = CatBoostClassifier(loss_function='Logloss', eval_metric='AUC')\n",
    "\n",
    "eval_pool = Pool(X_test_split, y_test_split, cat_features=cats)\n",
    "\n",
    "# clf = CatBoostClassifier(loss_function='Logloss',cat_features=cats,eval_metric= 'AUC',depth= 1,learning_rate= 1,l2_leaf_reg= 5,iterations= 2000)\n",
    "\n",
    "clf = CatBoostClassifier(depth=1, learning_rate=1, iterations=2000, stratified=True)\n",
    "\n",
    "fit(X_train_pre, y_train, clf, eval_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'learning_rate': [0.03, 0.1, 1],\n",
    "        'depth': [1, 2, 4, 6, 10],\n",
    "        'l2_leaf_reg': [1, 3, 5],\n",
    "        'iterations': [50, 100, 150, 500, 1000, 2000]}\n",
    "\n",
    "model.grid_search(grid,train_dataset, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier(max_depth=6, eta=.125, objective='binary:logistic', use_label_encoder=False)\n",
    "\n",
    "fit(X_train_split, y_train_split, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "clf = LGBMClassifier()\n",
    "fit(X_train_split, y_train_split, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, StackingClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=1000)\n",
    "fit(X_train_split, y_train_split, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pre.loc[~(X_train_pre['f18'] == X_train_pre['f26'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "models['Logistic Regression'] = LogisticRegression()\n",
    "\n",
    "# Decision Trees\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "models['Decision Trees'] = DecisionTreeClassifier()\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "models['Random Forest'] = RandomForestClassifier()\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "models['Naive Bayes'] = GaussianNB()\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "models['K-Nearest Neighbor'] = KNeighborsClassifier()\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "models['Catboost'] = CatBoostClassifier(iterations=2000, depth=1, learning_rate=1, verbose=False)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "models['Logistic Regression'] = LogisticRegression()\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "models['Light GBM'] = LGBMClassifier()\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "models['GBM'] = GradientBoostingClassifier()\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "accuracy, precision, recall, auc = {}, {}, {}, {}\n",
    "\n",
    "for key in models.keys():\n",
    "    \n",
    "    # Fit the classifier model\n",
    "    models[key].fit(X_train_split, y_train_split)\n",
    "    \n",
    "    # Prediction \n",
    "    predictions = models[key].predict_proba(X_test_split )\n",
    "    \n",
    "    # Calculate AUC\n",
    "    auc[key] = roc_auc_score(y_test_split, predictions[:, 1])\n",
    "    \n",
    "\n",
    "df_model = pd.DataFrame(index=models.keys(), columns=['Auc'])\n",
    "df_model['Auc'] = auc.values()\n",
    "\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanderlindberg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Wall time: 3min 35s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=StackingClassifier(cv=10,\n",
       "                                                estimators=[('lgbm',\n",
       "                                                             LGBMClassifier(device='gpu',\n",
       "                                                                            random_state=42))],\n",
       "                                                final_estimator=LogisticRegression()),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'lgbm__max_depth': [6, 7],\n",
       "                                        'lgbm__num_leaves': [70, 80]},\n",
       "                   scoring='roc_auc', verbose=10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, StackingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "models = {}\n",
    "# models['rf'] = RandomForestClassifier(random_state=42)\n",
    "# models['Catboost'] = CatBoostClassifier(boosting_type='Plain', gpu_cat_features_storage = 'CpuPinnedMemory', max_ctr_complexity=1, iterations=1000, depth=1, learning_rate=1, verbose=False, random_state=42, task_type=\"GPU\", devices='0:1')\n",
    "models['lgbm'] = LGBMClassifier(random_state=42, device='gpu')\n",
    "# models['gbm'] = GradientBoostingClassifier(min_samples_split=500,min_samples_leaf=50,max_depth=8, subsample=0.8, random_state=42)\n",
    "# models['xgb'] = XGBClassifier(tree_method='gpu_hist', gpu_id=0, verbosity = 0)\n",
    "clf = StackingClassifier(estimators = list(models.items()), final_estimator=LogisticRegression(), cv=10)\n",
    "\n",
    "params = {# 'rf__n_estimators': [5, 10, 100], \n",
    "          'lgbm__max_depth': [6,7], \n",
    "          'lgbm__num_leaves': [70, 80], \n",
    "          # 'gbm__n_estimators': range(20,81,10), \n",
    "          # 'gbm__learning_rate': [1, .1, .01]\n",
    " #            'xgb__max_depth': [5, 10, 20],\n",
    " #            'xgb__n_estimators': [10, 100, 1000],\n",
    " #            'xgb__learning_rate': [1, .1, .01]\n",
    "        }\n",
    "\n",
    "# grid = GridSearchCV(estimator=clf, param_grid=params, cv=5, n_jobs=-1, scoring='roc_auc', refit=True, verbose=100)\n",
    "grid = RandomizedSearchCV(estimator=clf, param_distributions=params, n_iter=10, cv=5, n_jobs=-1, scoring='roc_auc', refit=True, verbose=10)\n",
    "grid.fit(X_train_pre, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 161 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, StackingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "models = {}\n",
    "models['rf'] = RandomForestClassifier(random_state=42)\n",
    "models['Catboost'] = CatBoostClassifier(boosting_type='Plain', gpu_cat_features_storage = 'CpuPinnedMemory', max_ctr_complexity=1, iterations=1000, depth=1, learning_rate=1, verbose=False, random_state=42, task_type=\"GPU\", devices='0:1')\n",
    "models['lgbm'] = LGBMClassifier(random_state=42, device='gpu')\n",
    "models['gbm'] = GradientBoostingClassifier(min_samples_split=500,min_samples_leaf=50,max_depth=8, subsample=0.8, random_state=42)\n",
    "models['xgb'] = XGBClassifier(tree_method='gpu_hist', gpu_id=0, verbosity = 0, objective='binary:logistic', silent=True)\n",
    "\n",
    "# clf = StackingClassifier(estimators = list(models.items()), final_estimator=LogisticRegression(), cv=10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "def grid(estimator, params, n_jobs=-1):\n",
    "    grid_cv = GridSearchCV(estimator=estimator, param_grid=params, cv=5, n_jobs=n_jobs, scoring='roc_auc', refit=True, verbose=10)\n",
    "    grid_cv.fit(X_train_pre, y_train)\n",
    "    return grid_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best params lgbm: {'boosting_type': 'dart', 'colsample_bytree': 0.64, 'learning_rate': 1, 'max_bin': 255, 'n_estimators': 32, 'num_leaves': 16, 'objective': 'binary', 'subsample': 0.7}\n",
      "Best score lgbm: 0.7322889897844772\n",
      "Wall time: 3.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lgbm_params = {\n",
    "        'learning_rate': [1],\n",
    "        'n_estimators': [24, 32, 52],\n",
    "        'num_leaves': [16], # large num_leaves helps improve accuracy but might lead to over-fitting\n",
    "        'boosting_type' : ['dart'], # for better accuracy -> try dart\n",
    "        'objective' : ['binary'],\n",
    "        'max_bin':[255], # large max_bin helps improve accuracy but might slow down training progress\n",
    "        'colsample_bytree' : [0.64],\n",
    "        'subsample' : [0.7],\n",
    "}\n",
    "\n",
    "lgbm_grid = grid(models['lgbm'], lgbm_params)\n",
    "print(f'Best params lgbm: {lgbm_grid.best_params_}')\n",
    "print(f'Best score lgbm: {lgbm_grid.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Best params lgbm: {'depth': 1, 'iterations': 2000, 'l2_leaf_reg': 1e-20, 'leaf_estimation_iterations': 10, 'loss_function': 'CrossEntropy'}\n",
      "Best score lgbm: 0.7598584828689188\n",
      "Wall time: 1h 19min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cat_params = {'iterations': [500, 1000, 2000],\n",
    "              'depth': [1, 4, 5, 6],\n",
    "              'loss_function': ['Logloss', 'CrossEntropy'],\n",
    "              'l2_leaf_reg': np.logspace(-20, -19, 3),\n",
    "              'leaf_estimation_iterations': [10],\n",
    "}\n",
    "\n",
    "cat_grid = grid(models['Catboost'], cat_params, 2)\n",
    "print(f'Best params lgbm: {cat_grid.best_params_}')\n",
    "print(f'Best score lgbm: {cat_grid.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4860 candidates, totalling 24300 fits\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_params = {'min_child_weight': [1, 5, 10],\n",
    "                'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "                'subsample': [0.6, 0.8, 1.0],\n",
    "                'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "                'max_depth': [3, 4, 5],\n",
    "              'n_estimators': [10, 100, 500, 1000],\n",
    "              'learning_rate': [1, .1, .01],\n",
    "}\n",
    "\n",
    "xgb_grid = grid(models['xgb'], xgb_params)\n",
    "print(f'Best params xgb: {xgb_grid.best_params_}')\n",
    "print(f'Best score xgb: {xgb_grid.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_clf_params = {\n",
    "    'depth': 1, \n",
    "    'iterations': 2000, \n",
    "    'l2_leaf_reg': 1e-20, \n",
    "    'leaf_estimation_iterations': 10, \n",
    "    'loss_function': 'CrossEntropy',\n",
    "    'boosting_type': 'Plain', \n",
    "    'gpu_cat_features_storage': 'CpuPinnedMemory',\n",
    "    'max_ctr_complexity':1,\n",
    "    'iterations':1000,\n",
    "    'depth':1, \n",
    "    'learning_rate':1, \n",
    "    'verbose':False, \n",
    "    'random_state':42, \n",
    "    'task_type':\"GPU\", \n",
    "    'devices':'0:1'\n",
    "}\n",
    "\n",
    "lgbm_clf_params = {\n",
    "    'boosting_type': 'dart', \n",
    "    'colsample_bytree': 0.64, \n",
    "    'learning_rate': 1, \n",
    "    'max_bin': 255, \n",
    "    'n_estimators': 32, \n",
    "    'num_leaves': 16, \n",
    "    'objective': 'binary', \n",
    "    'subsample': 0.7,\n",
    "    'random_state':42, \n",
    "    'device':'gpu'\n",
    "}\n",
    "\n",
    "stacking_models = {}\n",
    "\n",
    "stacking_models['cat'] = CatBoostClassifier(**cat_clf_params)\n",
    "stacking_models['lgbm'] = LGBMClassifier(**lgbm_clf_params)\n",
    "\n",
    "clf = StackingClassifier(estimators = list(stacking_models.items()), final_estimator=LogisticRegression(), cv=10)\n",
    "fit(X_train_pre, y_train, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv': 5,\n",
       " 'error_score': nan,\n",
       " 'estimator__cv': 10,\n",
       " 'estimator__estimators': [('lgbm',\n",
       "   LGBMClassifier(device='gpu', random_state=42))],\n",
       " 'estimator__final_estimator__C': 1.0,\n",
       " 'estimator__final_estimator__class_weight': None,\n",
       " 'estimator__final_estimator__dual': False,\n",
       " 'estimator__final_estimator__fit_intercept': True,\n",
       " 'estimator__final_estimator__intercept_scaling': 1,\n",
       " 'estimator__final_estimator__l1_ratio': None,\n",
       " 'estimator__final_estimator__max_iter': 100,\n",
       " 'estimator__final_estimator__multi_class': 'auto',\n",
       " 'estimator__final_estimator__n_jobs': None,\n",
       " 'estimator__final_estimator__penalty': 'l2',\n",
       " 'estimator__final_estimator__random_state': None,\n",
       " 'estimator__final_estimator__solver': 'lbfgs',\n",
       " 'estimator__final_estimator__tol': 0.0001,\n",
       " 'estimator__final_estimator__verbose': 0,\n",
       " 'estimator__final_estimator__warm_start': False,\n",
       " 'estimator__final_estimator': LogisticRegression(),\n",
       " 'estimator__n_jobs': None,\n",
       " 'estimator__passthrough': False,\n",
       " 'estimator__stack_method': 'auto',\n",
       " 'estimator__verbose': 0,\n",
       " 'estimator__lgbm': LGBMClassifier(device='gpu', random_state=42),\n",
       " 'estimator__lgbm__boosting_type': 'gbdt',\n",
       " 'estimator__lgbm__class_weight': None,\n",
       " 'estimator__lgbm__colsample_bytree': 1.0,\n",
       " 'estimator__lgbm__importance_type': 'split',\n",
       " 'estimator__lgbm__learning_rate': 0.1,\n",
       " 'estimator__lgbm__max_depth': -1,\n",
       " 'estimator__lgbm__min_child_samples': 20,\n",
       " 'estimator__lgbm__min_child_weight': 0.001,\n",
       " 'estimator__lgbm__min_split_gain': 0.0,\n",
       " 'estimator__lgbm__n_estimators': 100,\n",
       " 'estimator__lgbm__n_jobs': -1,\n",
       " 'estimator__lgbm__num_leaves': 31,\n",
       " 'estimator__lgbm__objective': None,\n",
       " 'estimator__lgbm__random_state': 42,\n",
       " 'estimator__lgbm__reg_alpha': 0.0,\n",
       " 'estimator__lgbm__reg_lambda': 0.0,\n",
       " 'estimator__lgbm__silent': 'warn',\n",
       " 'estimator__lgbm__subsample': 1.0,\n",
       " 'estimator__lgbm__subsample_for_bin': 200000,\n",
       " 'estimator__lgbm__subsample_freq': 0,\n",
       " 'estimator__lgbm__device': 'gpu',\n",
       " 'estimator': StackingClassifier(cv=10,\n",
       "                    estimators=[('lgbm',\n",
       "                                 LGBMClassifier(device='gpu', random_state=42))],\n",
       "                    final_estimator=LogisticRegression()),\n",
       " 'n_iter': 10,\n",
       " 'n_jobs': -1,\n",
       " 'param_distributions': {'lgbm__max_depth': [6, 7],\n",
       "  'lgbm__num_leaves': [70, 80]},\n",
       " 'pre_dispatch': '2*n_jobs',\n",
       " 'random_state': None,\n",
       " 'refit': True,\n",
       " 'return_train_score': False,\n",
       " 'scoring': 'roc_auc',\n",
       " 'verbose': 10}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack Model accuracy score: 0.7689\n"
     ]
    }
   ],
   "source": [
    "prediction = predict_proba(clf, X_test_split)\n",
    "\n",
    "print_accuracy(prediction, y_test_split, 'stack')\n",
    "\n",
    "\n",
    "real_pred = predict_proba(clf, X_test_pre)\n",
    "\n",
    "f = open(\"./pred.csv\", \"w\")\n",
    "f.write(\"id,target\\n\")\n",
    "id_nr = 50000\n",
    "for v in real_pred[:, 1]:\n",
    "    f.write(f\"{id_nr},{v}\\n\")\n",
    "    id_nr += 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, cv\n",
    "\n",
    "cv_dataset = Pool(data=X_train_pre,\n",
    "                  label=y_train,\n",
    "                  cat_features=list(set(X_train_pre.columns).intersection(set(categorical_columns))))\n",
    "\n",
    "params = {\"iterations\": 2000,\n",
    "          \"depth\": 2,\n",
    "          \"learning_rate\": 1,\n",
    "          \"loss_function\": \"Logloss\",\n",
    "          \"verbose\": False,\n",
    "          \"roc_file\": \"roc-file\"}\n",
    "\n",
    "scores = cv(cv_dataset,\n",
    "            params,\n",
    "            fold_count=2, \n",
    "            plot=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
